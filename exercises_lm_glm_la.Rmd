---
title: "lm and glm and la course"
author: "Steven Wink"
date: "8 november 2016"
output: html_document
---
course part of master "Mathematics: Statistical Science for the Life and Behavioural Sciences, 2016-2017"  

### linear models, general linear models and linear algebra  
## This course is part of the master provided by the mathematics institute of the university of Leiden    

## course material  
* Fox, J (2008), Applied Regression Analysis and Generalized Linear Models, Sage  
* Faraway, J.J. (2002), Practical Regression and Anova using R - web text: https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf   
* Faraway, J.J. (2005) Extending the Linear Model with R, Chapman & Hall/ CRC
* Optional: McCulloch, C.E., Searle, S.R., and Neuhaus, J.M. (2008), Generalized, Linear, and Mixed Models, Wiley  

Note to self: There are newer versions of these books, check if OK to use the newer versions  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercises week 1 day 1  

```{r install load and explore packages faraway and car }
install.packages(c("faraway", "car"))
require(faraway)
require(car)

ls("package:faraway")
ls("package:car")
?qqPlot
help(package = "faraway") # functions and datasets for Books by Julian Faraway
help(package = "car") # companion to applied regression: Functions and Datasets to Accompany J. Fox and S. Weisberg, An R Companion to Applied Regression, Second Edition, Sage, 2011.

```


```{r 2. Follow analysis pages 10-13}
data(pima)
head(pima)
summary(pima)
sort(pima$diastolic)

 pima$diastolic[pima$diastolic == 0] <- NA
 pima$glucose[pima$glucose == 0] <- NA
 pima$triceps[pima$triceps == 0] <- NA
 pima$insulin[pima$insulin == 0] <- NA
 pima$bmi[pima$bmi == 0] <- NA

pima$test <- factor(pima$test)
summary(pima) 
hist(pima$diastolic)
plot(density(pima$diastolic,na.rm=TRUE))
plot(sort(pima$diastolic),pch=".")
plot(diabetes ~ diastolic, pima)
plot(diabetes ~ test,pima)

pairs(pima, col = pima$test)

```

```{r Exercise 3 faraway book }

# a numeric and graphical summary of prostate dataset
head(prostate)
summary(prostate)
pairs(prostate)

```


```{r Exercise D2.1 (fox) }
# a)
data(Sahlins)
head(Sahlins)
plot( Sahlins$acres ~Sahlins$consumers)

# weak positive linear relation
# two outliers, at ~(1.2, 3.1) and at ~(1.6, 1.3)

# b)
Sahlins$group <- c(rep("g1", 7), rep("g2", 6), rep("g3", 7))
groupMeans <-lapply( 
    split(Sahlins, Sahlins$group), 
    function(x) colMeans(x[, 1:2])) 
groupMeans <- t( as.data.frame(groupMeans) )
points(groupMeans, col = c("blue", "green", "red"))
legend( 2.1,3, legend = rownames(groupMeans), fill =  c("blue", "green", "red"), pch = ".")

# connect the group means with a simple nonparametric regression line:
lines(groupMeans)
# does not help to interpret relation between these two variables, no.

# c)
# remove outliers from g1 and g2:

Sahlins <- Sahlins[-4, ]
rownames(Sahlins) <- 1: nrow(Sahlins)
Sahlins <- Sahlins[-11, ]
rownames(Sahlins) <- 1: nrow(Sahlins)


groupMeans <-lapply( 
    split(Sahlins, Sahlins$group), 
    function(x) colMeans(x[, 1:2])) 
groupMeans <- t( as.data.frame(groupMeans) )
plot( Sahlins$acres ~Sahlins$consumers)
points(groupMeans, col = c("blue", "green", "red"))

legend( 2.1,3, legend = rownames(groupMeans), fill =  c("blue", "green", "red"), pch = ".")
lines(groupMeans)
# shifted away from outliers, relation between variables now more apparent

# d) locally weighted regression
lines( lowess( Sahlins$acres ~ Sahlins$consumers), col = "pink") 

```

```{r D2.2 Robey}
# a)
data(Robey)
head(Robey)
plot( Robey$tfr ~ Robey$contraceptors)

# strong negative linear relation
# Not really

# b)
Robey <- Robey[ order(Robey$contraceptors), ]
Robey$group <- c(rep("g1", 17), rep("g2", 16), rep("g3", 17))
groupMeans <-lapply( 
    split(Robey, Robey$group), 
    function(x) colMeans(x[, 3:2])) 

groupMeans <- t(as.data.frame(groupMeans) )

points(groupMeans, col = c("blue", "green", "red"))

legend( 60,7, legend = rownames(groupMeans), fill =  c("blue", "green", "red"), pch = ".")

# connect the group means with a simple nonparametric regression line:
lines(groupMeans)
# does not help to interpret relation between these two variables, no.

# c)
# remove outliers from g1 and g2:


# d) locally weighted regression
lines( lowess( Robey$tfr ~ Robey$contraceptors), col = "pink") 

```

chapter 3
```{r D3.1 }
# distributions. Symmetry, skewness, non-normality/ apparent normality, number of modes and presence or absence of unusual values
# histogram, qq plots boxplots scatterplots, pairs
head(Freedman)
hist(Freedman$population) # right tailed, no normality
hist(Freedman$nonwhite)# right tailed, no normality
hist(Freedman$density)# right tailed, no normality
hist(Freedman$crime) # normally distr, symmetrical

qqPlot(Freedman$population)# heavy right tailed. light left tailed
qqPlot(Freedman$nonwhite)#heavy right tailed. light left tailed
qqPlot(Freedman$density)# heavy right tailed. light left tailed
qqPlot(Freedman$crime) # normal

boxplot(Freedman) # same
plot(Freedman)
pairs(Freedman)

```


```{r exerc 7 }
rsample <- rnorm(20, mean = 170, sd = 10)
qqPlot(rsample)
rsample <- rnorm(200, mean = 170, sd = 10)
qqPlot(rsample)

```


```{r D3.4}
head(Burt)
iqLow <- lm(IQbio ~IQfoster, data = Burt[Burt$class =="low",])
iqMedium <- lm(IQbio ~IQfoster, data = Burt[Burt$class =="medium",])
iqHigh <- lm(IQbio ~IQfoster, data = Burt[Burt$class =="high",])
plot(Burt$IQfoster, Burt$IQbio)
abline(iqLow, col = "red")
abline(iqMedium, col = "green")
abline(iqHigh, col = "blue")

# yes, the slopes are too similar for a random sample dataset
```

Chapter 4

```{r D4.1 }
head(Freedman)
#population: Total 1968 population, 1000s.
#nonwhite: Percent nonwhite population, 1960.
#density: Population per square mile, 1968.
#crime: Crime rate per 100,000, 1969.

# nonwhite and crime are proportions

plot(density(Freedman$population, na.rm = TRUE))
qqPlot(Freedman$population)

# mainly shrink the right tail and expand the close to 0 values.
plot(density(log(Freedman$population), na.rm = TRUE))
qqPlot(log(Freedman$population))
# log is still too skewed. Try a different power rule
# Box-Cox transformation: with p 0 < 1
p <- -0.8
plot(density( 
  ((Freedman$population^p -1) / p), na.rm = TRUE))

qqPlot(((Freedman$population^p -1) / p))

p <- -0.8
plot(density( 
  Freedman$population^p , na.rm = TRUE))
qqPlot(Freedman$population^p)

# non white


plot(density(Freedman$nonwhite, na.rm = TRUE))
qqPlot(Freedman$nonwhite)

plot(density(log(Freedman$nonwhite), na.rm = TRUE))
qqPlot(log(Freedman$nonwhite))# log is fine

# now consider a logit, since it is a proportional value

plot(density(
      log(Freedman$nonwhite/100) / ( 1- (Freedman$nonwhite)/100)
  , na.rm = TRUE))

qqPlot( log(Freedman$nonwhite/100) / ( 1- (Freedman$nonwhite)/100))# log is fine
# also works

# density

plot(density(Freedman$density, na.rm = TRUE))
qqPlot(Freedman$density)

plot(density(log(Freedman$density), na.rm = TRUE))
qqPlot(log(Freedman$density))
# log fine

# crime
plot(density(Freedman$crime, na.rm = TRUE)) # already normal
qqPlot(Freedman$crime)

# already good

# consider using logit transformation (0-1 --> -inf - inf)

plot(density(
      log(Freedman$crime/100000) / ( 1- (Freedman$crime)/100000)
  , na.rm = TRUE))

qqPlot( log(Freedman$crime/100000) / ( 1- (Freedman$crime)/100000))# log is fine

# logit made it worse


```


```{r UN}
head(UN)
help(UN)
boxplot(UN$infant.mortality) # fraction infant deaths (per 1000)

qim <- quantile(UN$infant.mortality, na.rm=TRUE)

ratioFun <- function(x) { 
qim <- quantile(x, na.rm = TRUE)
  result <-  (qim[4] - qim[3]) / (qim[3]-qim[2])
  names(result) <- NULL
  return(result)
  }

ratioFun(UN$infant.mortality) # 2

# logit
p <- UN$infant.mortality /1000
ratioFun(log( p / (1-p)) ) # 0.88
boxplot(log( p / (1-p)))
plot(density(log( p / (1-p)), na.rm=TRUE))
plot(density(p, na.rm=TRUE))

# Box-Cox transformation
UN$infant.mortality

boxCoxFun <- function(x, p) {
transfx <-  (x^p - 1 / p)
transfx
}

for(i in 1:10){
  p <- seq(from = -1, to =1 , length.out=10)[i]
ratioOut <- ratioFun(
  boxCoxFun(UN$infant.mortality, p)

  )
print(paste0("p = ", p))
print(ratioOut)
}

boxplot(boxCoxFun(UN$infant.mortality, 0.11))
plot(density(boxCoxFun(UN$infant.mortality, 0.11), na.rm=TRUE))



```



```{r 11 Prestige }

head(Prestige)
plot(Prestige$prestige ~ Prestige$income)
lines(lowess(Prestige$prestige ~ Prestige$income))

# 4rth quadrant: downsize y, increase x
plot(Prestige$prestige ~ sqrt(Prestige$income))
lines(lowess(Prestige$prestige ~ sqrt(Prestige$income)))
lm(Prestige$prestige ~ sqrt(Prestige$income))
abline(a = 2.17, b = 0.564, lty = 2)

``` 

```{r 12 }
head(UN)
plot(UN$infant.mortality ~ UN$gdp)
plot(log(UN$infant.mortality) ~ log(UN$gdp))
abline(lm(log(UN$infant.mortality) ~ log(UN$gdp)), lty = 2)
UN <- na.omit(UN)
lines(lowess(log(UN$infant.mortality) ~ log(UN$gdp)))

``` 


```{r 13}

x <- seq(0.01, 0.99, length.out=100)
ylogit <- log( x / (1-x) )
plot(x,ylogit) # -inf and + inf near 0 and 1, respectively 

yprobit <- qnorm(x)

points(x, yprobit, col  ="red")

yarcsinsqrt <- (asin(sqrt(x)))

points(x,yarcsinsqrt, col = "green")
```


## Exercises week 1 day 2  
Exercises chapter 5
```{r 1 }

# a) by hand: B = 3/2, A = 1
# b)
XY <- data.frame(X=c(1, 2, 3), Y = c(2, 5, 5))
# A = Ybar - B*Xbar
# B = sum( (XY$X - mean(XY$X)) * (XY$Y - mean(XY$Y)) )
B <- sum( (XY$X - mean(XY$X)) * (XY$Y - mean(XY$Y)) ) / sum( (XY$X - mean(XY$X))^2 )
A <- mean(XY$Y) - B * mean(XY$X)

# c)
lm(Y~X, data = XY)

```


```{r D5.1}
#a) 
Sahlinsub <- Sahlins[ c(1,5,10,15,20),]
plot(Sahlinsub$consumers, Sahlinsub$acres)
#b)
SahRegr <- lm( Sahlinsub$acres ~ Sahlinsub$consumers)
abline( lm( Sahlinsub$acres ~ Sahlinsub$consumers) )
# A is the intercept, when zero consumers are in a household the acres of land of the household estimation
# Per unit of consumer per gardener, an increase of 0.4 acres per gardener exists based on the model of this subset of the data.

#c)
n = nrow(Sahlinsub)
k = 1

Yhat <- predict(SahRegr)
A <- coefficients(SahRegr)[1] 
B <- coefficients(SahRegr)[2] 
A + B* Sahlinsub$consumers
RSS <- sum((Sahlinsub$acres - Yhat)^2)
(SE <- sqrt( RSS / ( n - (k+1) )))
RSS <- sum((Sahlinsub$acres - Yhat)^2)

 TSS <- sum((Sahlinsub$acres - mean( Sahlinsub$acres ))^2) # from the null model with only intercept
 RSS
 
RegSS <- TSS - RSS
RSq <- RegSS / TSS 
r <- sqrt(RSq) # slope of regression is positive
 # the Standard error of the regression is the goodness of fit of the regression to the data. 

``` 


```{r D5.3}
# find A, B, SE and r
?Freedman
head(Freedman)
# response variable: crime
# explanatory variables: population & nonwhite & density
plot( crime ~ (population), data = Freedman)
lmo <- lm( crime ~ population, data = Freedman  )
abline(lmo) 
coefficients(lmo)
summary(lmo)
deviance(lmo)
length(predict(lmo))
indNA <- is.na(Freedman$crime) | is.na(Freedman$population)

RSS <- sum((predict(lmo) - Freedman$crime[!indNA])^2) 

k=1
n = sum(!indNA)
SE <- sqrt( RSS / (n - (k+1)))  # standard deviation of residuals
TSS <- sum((Freedman$crime[!indNA] - mean(Freedman$crime[!indNA]))^2)
RegSS <- TSS - RSS

Rsq <- RegSS / TSS
r <- sqrt(Rsq)


```


```{r D5.5}
head(States)
?States
# response variable: pay (average teachers pay per state)
# explanatory variables: pop, SATV, SATM, percent & dollars

# calculate least square regression of the response on the explanatory variables
# interprete the A and Bj's obtained
# calculate standard error of the regression SE
# calculate the multiple-correlation coefficient


# first the single regressions for this dataset

plot( pay ~ dollars, data = States)
lmo <- lm( pay ~ dollars, data = States  )
abline(lmo) 
(RSS <- sum((predict(lmo) - States$pay)^2) )
k=1
n = nrow(States)
(SE <- sqrt( RSS / (n - (k+1))) ) # standard deviation of residuals
(TSS <- sum((States$pay - mean(States$pay))^2))
(RegSS <- TSS - RSS)

(Rsq <- RegSS / TSS)
(r <- sqrt(Rsq))
summary(lmo)
# summary single regressors:
# pop: intercept: 29,  slope: 3.59e-4, SE: 5, r: 0.368
# SATV:  intercept: 73.8, slope: -0.096, SE: 4.46, r: 0.56
#SATM: intercept: 68, slope: -0.0745 , SE:4.69  , r: 0.485 
#percent intercept: 26, slope: 0.146, SE: 4.014, r: 0.66
#dollars: intercept 14 , slope: 3.27 , SE: 2.84 , r: 0.847 

# now for the multiple regression

lmo <- lm( pay ~ pop + SATV + SATM + percent + dollars, data = States  )
summary(lmo)
(RSS <- sum((predict(lmo) - States$pay)^2) )
k=5
n = nrow(States)

(SE <-   sqrt( RSS / (n - (k+1))) ) # standard deviation of residuals

(TSS <- sum((States$pay - mean(States$pay))^2))
(RegSS <- TSS - RSS)

(Rsq <- RegSS / TSS)
(r <- sqrt(Rsq))

# the intercept is different - there is 1 slope for the multiple regressor model
# the regression coefficients of the regressors are different compared to the single regression slopes.
# the standard deviation of the residuals is lower (as expected since more flexible model)
# The multiple correlation coefficient r is higher

```

```{r 5.4 }
#a) When X' = X - 10
# A = Ybar - B*(Xbar-10) ->
# A = Ybar - B*Xbar +10B

#A' is A + 10*B   
#B' is B
#SE' is SE  (sqrt(RSS/(n-(k+1))) & RSS = sum(Yhat - Y) ^2) and y and yhats do not change
#r' since r = sqrt( RegSS/ TSS ) and RegSS = TSS - RSS. TSS = sum((y-yhats)^2) is same and RSS also
# r =r`

# for X' = 10X
# A = A' : intecept is unchanged
# B' = 1/10 * B: the slopes are 10-fold lower
#SE`= SE unchanged
#r' = r also unchanged

# for X' = 10*(X+1)
#A' is A + 10*B
# SE' and r' unchanged


#b) Y`` = Y + 10
# A`` = A + 10
# B`` = B unit-change of y per unit-change of x remains the same
# SE`` = SE = sqrt( RSS/ n - (k+1)); RSS = sum((Yhat`` - Y+10)^2); ends up in 10-10
# r`` = r

# Y`` = 5Y
# B`` = 5*B For every unit change in X, the Y changes 5 times as much
# A`` = 5*Ybar - 5*B*Xbar
# SE`` = 5*SE  ; RSS`` = sum((5*Yhat - 5*Y)^2) = 5^2 * RSS; SE = sqrt(25*RSS/(n-(k+1)))
# r`` = r;  RegSS/ TSS; RegSS = TSS - RSS. TSS = sum((5*Y-5*Ybar)^2) so cancels out

# Y``` = 5Y + 10

# A`` = 5*Yvar - 5*B*Xbar + 10
# B`` = 5*B
# SE`` = 5*SE
# r`` = r



# c) with linear transformation of X: alpha*X + beta
# intercept changes by: A = A - beta
# slope changes by 1/alpha
# SE never changes by linear transformations on X
#r never changes by linear transformations on X

# with linear transformation of Y: alpha*Y + beta
# intercept changes by: A = A + beta
#slope changes by: alpha*B
# SE changes by alpha*SE
# r does not change

```


```{r 5.7}
head(Prestige)
#a) 
# first the `normal` B1 regression slope:
lmoN <- lm(prestige ~ education + income + women, data = Prestige)
# slope for education is ~ 4.1867
# regress prestige on X2:X3 (the other explanatory residuals than the one we will calc the slope on)
lmoR <- lm(prestige ~ income + women, data = Prestige)
# regress X1 on the other regressors:
regrX2X3 <- lm(education ~ income + women, data = Prestige)
# regres the residuals of y on the residuals of the regressors:
regrRes <- lm( residuals(lmoR) ~ residuals(regrX2X3))
# the slope is 4.187 which is consistent with the `normal` regression

#b)
# because the residuals of regressor and regressed vary around mean zero

#c)
# "B1 by this method is the effect of X1 on Y after the influence of other regressors on Y have been removed"
# since the residuals after the regression of the X2 and X3 on Y is the remainder of variation of Y after removal of influence on X2 and X3 on Y, and the residuals of regression of X2 and X3 on X1 removes the influence of X2 and X3 on X1, this would indeed be logical to say.
``` 

```{r exercise_4}

head(prostate)
# response variable: lpsa, predictor lcavol
# residuals standard deviation:
lmo <- lm( lpsa ~ lcavol, data = prostate)
summary(lmo)$r.squared
summary(lmo)$sigma
# res stand deviation: 0.7875
# rs = 0.5394
lmo2 <- lm( lpsa ~ lcavol + lweight, data = prostate)
lmo3 <- lm( lpsa ~ lcavol + lweight + svi, data = prostate)
lmo4 <- lm( lpsa ~ lcavol + lweight + svi + lbph, data = prostate)
lmo5 <- lm( lpsa ~ lcavol + lweight + svi + lbph + age, data = prostate)
lmo6 <- lm( lpsa ~ lcavol + lweight + svi + lbph + age + lcp, data = prostate)
lmo7 <- lm (lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45, data = prostate)
lmo8 <- lm( lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45 + gleason, data = prostate)
SE <- c(summary(lmo)$sigma, summary(lmo2)$sigma, summary(lmo3)$sigma, summary(lmo4)$sigma
        ,summary(lmo5)$sigma,summary(lmo6)$sigma,summary(lmo7)$sigma,summary(lmo8)$sigma)

Rsq <- c( summary(lmo)$r.squared, summary(lmo2)$r.squared,summary(lmo3)$r.squared,summary(lmo4)$r.squared,summary(lmo5)$r.squared,summary(lmo6)$r.squared,summary(lmo7)$r.squared,summary(lmo8)$r.squared)
plot(1:8, SE, ylim = c(0.4,1), main =c("SE resid (dots)", "Rsq (red)"))
lines(1:8, Rsq, col = 'red')

```

```{r exercise_5}
plot( lpsa ~ lcavol, data = prostate)
lmo <- lm( lpsa ~ lcavol, data = prostate)
abline(lmo, col ="red")
lmo2 <- lm( lcavol ~ lpsa, data = prostate)
abline(lmo2, col ="blue") # need to switch the y and x axis;
lines(x = lmo2$fitted.value, y = prostate$lpsa, col = "green")
# looks like they intercept at the corresponding means. Makes sense because the liner regressions do pass through the means of Y and X.
points(mean(prostate$lcavol), y = mean(prostate$lpsa), pch = 10, col = "red", cex = 2)
      
``` 

```{r exercise_6.6}
#a) 
# V(B)=1/sum(xi-x_bar)^2  * SE_resid the denominator is increased by 100 times thus the sqrt (to get the SD) is 10 times, the standard deviation of the errors do not change because these are the residuals in the y-direction. Thus the V(B`) is 10 times lower than v(B)
# however the t0` = B`/SE(B`) is the same, as both the B (coefficient) and SE decrease by a factor of 10

#b) 
#Now the y is linearly transformed; Y`` = 5(Y + 2)
# the variance of the residuals is 25 times larger (the vertical shift doesnt matter as (y + 2) - (y_hat + 2) the 2 dissapears. the SE(B``) under this transformation thus leads to 5X higher value.
# for the t0`` = B`` / SE(B``): the B (slope) is 5 times large, as is the SE, thus again no change. 

#c)
# under linear transformation of X` = aX + b, with B the slope; the SE(B`) = 1/a * SE(B)
# under linear transformation of Y` = aY + b, with B the slope; the SE(B``) = a*SE(B)
# the t0 is invariant to linear transformations of X and/ or Y

```

```{r exercise_6.7}
# fit the full model and fit a reduced model. Compare the difference in RegSS in an F-test
# F = [] (RegSS - RegSS0) / q ] / [ RSS / ( n - (k+1)) ]
# think of the single model case with F = RegSS/k  / RSS/(n-(k+1)): with the H0 that slope is zero the RegSS is equal to the Residual SS. RegSS = SS - RSS, or RegSS = sum(Yhat-Ybar)^2 (residuals compared to mean)
# likely only meaningfull with identical units of the explanatory variables of which the coefficients are tested for equality.

# perform the F-test `manually` and with anova function in R;

#H0: b1=b2
head(Prestige)

lmoF <- lm(prestige ~ education + income, data = Prestige)
# create eduction+income column;
Prestige$eduincome <- Prestige$education + Prestige$income
lmo0 <- lm( prestige ~ eduincome, data = Prestige )
anova(lmoF, lmo0) # F = 140.42

# manually:
names(summary(lmoF))
names(lmoF)
RSS <- deviance(lmoF) # RSS
RegSS <- sum((lmoF$fitted.values - mean(Prestige$prestige))^2) # RegSS
TSS <- sum((Prestige$prestige - mean(Prestige$prestige))^2) # TSS
TSS-RSS == RegSS

RegSS0 <- sum((lmo0$fitted.values - mean(Prestige$prestige))^2)
q = 1 # difference of 1 variable between the null and full model
n = nrow(Prestige)
k= 2
(Fmanual <- ( (RegSS - RegSS0 ) / q )/   ( RSS/(n-(k+1))   ) ) # 140.42

# We can safely reject the hypothesis of equal coefficients; because the null model is significanly different than the full model: compared to the residual SS there is a huge difference in the regressed/ explained variance of Y using equal coefficients as compared to non-equal.

```

```{r faraway_text28_33 }

data(savings)
head(savings)

g <- lm( sr ~ pop15 + pop75 + dpi + ddpi, data = savings )
summary(g)

sum((savings$sr - mean(savings$sr))^2) # TSS
983.628
sum(g$residuals^2)
650.713
TSS - RSS / 4   /   RSS/n  # F statistic
# TSS - RSS is the amount explained by full model.

g2 <- lm(sr ~ pop75 + dpi + ddpi, data = savings)
sum(g2$residuals^2)
797.7 - 650/ 1  # change in RSS
147.7 / (650.7 /45)
1- pf(10.167, 1,45)


```

```{r farawayBook_51}
head(sat)
lmo <- lm( total ~ expend + ratio + salary, data = sat)
# test beta_salary = 0
lmo0 <- lm( total ~ expend + ratio, data = sat)
anova(lmo, lmo0)
# fail to reject. Likely can be removed.

# test all coef are 0
TSS <- sum((sat$total - mean(sat$total))^2) 
RSS <- deviance(lmo)
sum(summary(lmo)$residuals^2)
(Fstat <- ((TSS - RSS)/3)  /  (RSS/ (50-(3+1)) ) )
1-pf(Fstat, 3, 46)
# the model does explain a part of the response variable as we can safely reject the null of zero coefficients
summary(lmo)
# yes, because we can reject the null that all coeff are zero as p = < 0.05
lmoT <- lm(total ~ expend + ratio + salary + takers, data = sat)
anova(lmoT, lmo)
# F >> 1. The change in residuals is large compared to full model resi. It is worth keeping takers in the model
plot(sat$takers, sat$total)

(TSS - deviance(lmoT) ) / 4 / (deviance(lmoT)/45)
summary(lmoT)

(summary(lmoT))
tstat <- -2.9045/0.2313
pt(tstat^2, 45)  #?


```


```{r D6.2}


```
